---
categories: [CNU Courses, Data Science]
---

# DS03

## 데이터 품질

- 데이터의 품질을 높이기 위해 **정제, 통합, 축소, 변환**을 한다.

## 데이터 품질을 결정 짓는 요소들

- 정확성(accuracy): 기록이 정확함
- 완전성(completeness): 기록이 완전히 사용 가능함
- 일관성(consistency): 기록의 편집이 일관적임
- 적시성(timeliness): 적시에 갱신됨
- 신뢰성(reliability): 신뢰할 수 있음
- 해석가능성(interpretability): 쉽게 이해 가능함.

## 데이터 전처리

- 데이터를 수집하는 과정과 modeling을 진행하는 과정 중간에 들어가는 과정이다.

### 데이터 정제(cleaning)

- 현실 세계의 데이터는 지저분함

#### 불완전 데이터(incomplete data) 

- 결측치(missing values)를 값으로 포함하는 데이터
- 장비 또는 센서의 오작동(malfunctioned)으로 인해 발생 가능
- 정보의 수집 거부로 발생 가능
- 속성 값의 적용 범위로 인해 발생 가능

##### 데이터 정제 방법

- Data imputation를 이용하여 결측치를 채워넣는다.
- 결측치를 "unknown" 또는 "N/A" 등으로 사용하는 것이 의미 있을 수 있음
- 결측치가 지나치게 많은 경우 해당 속성을 분석해서 제거하는 것을 고려 가능
- 전체 속성 값의 평균 또는 동일 클래스의 속성 값의 평균으로 대체 가능
- 추론을 통해 채워 넣을 수 있음

#### 노이즈가 있는 데이터(noisy data)

- 에러(errors) 또는 이상치(outliers)를 포함하는 데이터
- 잡음(noise)은 측정 값의 오차(error), 데이터의 형태를 바꿀 수 있음
- 결함이 있는 수집 장치, 전송의 어려움, 기술적 한계 등으로 발생

##### 데이터 정제 방법 

- 노이즈가 있는 데이터 보완하기
- 구간화(binning): 값의 정렬 후 구간화, 구간의 평균, 중간값 등으로 평탄화(smoothing)
- 군집화(clustering): 값의 군집화를 통한 이상치 검출 및 제거
- 데이터 검수: 사람이 검수하여 의심되는 값 파악 및 처리
- 이동 평균(moving average) : 일정 구간에서의 값들을 평균값으로 대체하여 노이즈를 상쇄하는 방법

#### 불일치 데이터(inconsistency data)

- 같아야 하는 값들 사이의 논리적 오류가 발생하는 데이터

##### 데이터 정제 방법

- 데이터 획득 과정을 재점검한다.

### 데이터 통합(integration)

- 다양한 소스로부터 수집된 데이터들을 통합하는 것이 중요
- 스키마 통합(schema integration)
  - 이종(heterogeneous) 스키마를 사용한 여러 소스의 데이터를 통합
- 엔티티 해결(entity resolution)
  - 여러 소스의 일치하는 기록(matching records)를 식별
- 불일치(inconsistent) 처리
  - 속성 값이 다른 경우 실제 값을 정함

## 피쳐 엔지니어링

### 데이터 축소(reduction)

- 대규모 데이터는 처리 시간이 상당히 소요될 수 있음
- 데이터 분석 결과, 성능을 유지하며 데이터셋 크기를 축소하는 작업
- ex)
  - 차원 축소(dimensionality reduction)
  - 표본 추출(sampling)
  - 압축(compression)

#### 차원 축소(dimensionality reduction)

- 차원의 저주(curse of dimensionality)
  - 차원이 증가할수록 공간 상의 데이터가 차지하는 공간이 상대적으로 희소(sparse)
  - 데이터 포인트의 밀도 및 거리에 대한 의미가 퇴색되어 군집화, 이상치 탐지 등이 잘되지 않음 
- 차원의 저주를 피하고 데이터 분석 기법의 수행 시간 및 메모리 사용량을 줄이기 위함
- 데이터를 더욱 효과적으로 시각화 가능
- ex)
  - 주성분분석(PCA)
  - 특징 선택

<h1 style="color: red; margin-top: 0">PCA 매우 중요!!! 시험 무조건 나올 수 있음!!!!</h1>

##### 주성분분석(PCA: Principal Component Analysis)

- 주성분(principal component): 데이터를 투영(projectioN) 할 때 분산이 가장 큰 벡터의 축
- 2번째, 3번째, ... 주성분은 제약조건 하에서 투영 시 가장 큰 분산을 갖도록 하는 벡터의 축
- **_투영된 데이터의 분산(variance)이 큰 것은 데이터의 설명력이 보존된다고 해석 가능_**
- 데이터 행렬 X의 각 열에 평균값을 빼서 평균을 0으로 맞춘 후 공분산 행렬 X<sup>T</sup>X의 고윳값이 큰 고유벡터(eigenvector)부터 순차적으로 주성분 선택 후 투영을 통한 차원 축소

##### 특징 선택(feature selection)

- 불필요하거나 관련없는 특징을 제거
- 불필요한(redundant) 특징 : 다른 속성들의 정보를 중복으로 가지는 속성
- 관련없는(irrelevant) 특징 : 주어진 문제에 대한 유용한 정보가 없는 속성
- 특징 선택 기법
  - 전진 선택(forward selection) : 공집합으로부터 유용한 특징을 순차적으로 추가
  - 후진 선택(backward selection) : 전체 집합에서 불필요한 특징을 순차적으로 제거
  - 의사결정나무(decision tree) : 풀고자 하는 문제에 영향력을 미치는 특징 선택

#### 표본추출(Sampling)

- 모집단(population)에 대한 지식을 얻기 위해 모집단의 부분집합을 선택하는 기법
- 데이터를 대표할 수 있는 부분 집합을 택하여 분석 기법의 복잡도를 줄이는 것이 목표
- 단순 랜덤 표본추출(simple random sampling): 특정 후보를 택할 확률이 모두 동일
- 비복원 표본추출(sampling without replacement): 선택된 개체를 제거하고 표본추출
  - ex) 로또 추첨
- 복원 표본추출(sampling with replacement): 선택된 개체를 제거하지 않고 표본추출
  - ex) 동전 던지기를 시행하여 앞면, 뒷면이 나오는 횟수 카운트
- 층화 표본추출(stratified sampling): 모집단을 그룹마다 비율이 유지되도록 표본추출
  - ex) 모집단을 여러 그룹으로 분할하고 각각의 비율이 유지되게 표본추출

#### 데이터 압축(compression)

- 기존의 데이터 표현보다 적은 비트(bit) 수를 사용하여 정보 부호화(encoding)
- 데이터를 저장하고 질의를 처리하는데 있어서 성능 개선 효과
- 런-렝스 부호화(run-length encoding) : 같은 값이 연속해서 나타나는 경우의 압축
- 사전 부호화(dictionary encoding) : 값에 대한 사전을 만들고 색인을 통해 압축

### 데이터 변환(transformation)

- 주어진 속성의 값들을 새로운 값으로 변환하여 사용
- ex)
  - 정규화(normalization)
  - 이산화(discretization)

#### 정규화(normalization)

- 최소-최대(min-max) 정규화 : 최솟값, 최댓값이 0,1이 되도록 정규화
  - v' = ( v - min<sub>A</sub> ) / ( max<sub>A</sub> - min<sub>A</sub> )
- Z-score 표준화(standardization): 평균, 표준편차가 0,1이 되도록 표준화
  - v' = ( v - μ<sub>A</sub> ) / σ<sub>A</sub>

#### 이산화(discretization)

- 레이블 부호화(label encoding): 속성 범위를 구간으로 나누고 실제 데이터 값을 구간 레이블로 대체
- 동일-간격(equal-width), 동일-빈도(equal-frequency) 등으로 나눌 수 있음
- 데이터 탐색을 통해서 적절한 레이블 부호화 기준을 택하여 사용 가능
 
